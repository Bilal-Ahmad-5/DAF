{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bilal-Ahmad-5/DAF/blob/main/DA'F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qjp1dGSnjMql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237b8dec-ea7f-4052-cc2a-3bea0cf4efaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas numpy tensorflow scikit-learn matplotlib seaborn transformers trl peft accelerate bitsandbytes tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsJT-6B0k6rn",
        "outputId": "8acd80df-9870-4b53-ee3d-aff3a1a109be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download path: /kaggle/input/productdemandforecasting\n",
            " -> Found CSV: /kaggle/input/productdemandforecasting/Historical Product Demand.csv\n",
            "\n",
            "INFO:\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1048575 entries, 0 to 1048574\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count    Dtype \n",
            "---  ------            --------------    ----- \n",
            " 0   Product_Code      1048575 non-null  object\n",
            " 1   Warehouse         1048575 non-null  object\n",
            " 2   Product_Category  1048575 non-null  object\n",
            " 3   Date              1037336 non-null  object\n",
            " 4   Order_Demand      1048575 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 40.0+ MB\n",
            "None\n",
            "\n",
            "DESCRIBE:\n",
            "\n",
            "        Product_Code Warehouse Product_Category       Date Order_Demand\n",
            "count        1048575   1048575          1048575    1037336      1048575\n",
            "unique          2160         4               33       1729         3828\n",
            "top     Product_1359    Whse_J     Category_019  2013/9/27        1000 \n",
            "freq           16936    764447           481099       2075       112682\n",
            "\n",
            "SHAPE:\n",
            "\n",
            "(1048575, 5)\n",
            "\n",
            "COLUMNS:\n",
            "\n",
            "Index(['Product_Code', 'Warehouse', 'Product_Category', 'Date',\n",
            "       'Order_Demand'],\n",
            "      dtype='object')\n",
            "\n",
            "DATA TYPES:\n",
            "\n",
            "Product_Code        object\n",
            "Warehouse           object\n",
            "Product_Category    object\n",
            "Date                object\n",
            "Order_Demand        object\n",
            "dtype: object\n",
            "\n",
            "UNIQUE VALUES:\n",
            "\n",
            "Product_Code        2160\n",
            "Warehouse              4\n",
            "Product_Category      33\n",
            "Date                1729\n",
            "Order_Demand        3828\n",
            "dtype: int64\n",
            "\n",
            "HEAD:\n",
            "\n",
            "   Product_Code Warehouse Product_Category       Date Order_Demand\n",
            "0  Product_0993    Whse_J     Category_028  2012/7/27         100 \n",
            "1  Product_0979    Whse_J     Category_028  2012/1/19         500 \n",
            "2  Product_0979    Whse_J     Category_028   2012/2/3         500 \n",
            "3  Product_0979    Whse_J     Category_028   2012/2/9         500 \n",
            "4  Product_0979    Whse_J     Category_028   2012/3/2         500 \n",
            "\n",
            "TAIL\n",
            ":\n",
            "         Product_Code Warehouse Product_Category       Date Order_Demand\n",
            "1048570  Product_1791    Whse_J     Category_006  2016/4/27        1000 \n",
            "1048571  Product_1974    Whse_J     Category_006  2016/4/27           1 \n",
            "1048572  Product_1787    Whse_J     Category_006  2016/4/28        2500 \n",
            "1048573  Product_0901    Whse_J     Category_023  2016/10/7          50 \n",
            "1048574  Product_0704    Whse_J     Category_001  2016/6/27           4 \n"
          ]
        }
      ],
      "source": [
        "import kagglehub, os, pandas as pd\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"felixzhao/productdemandforecasting\")\n",
        "print(\"Download path:\", path)\n",
        "\n",
        "# List all files\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for f in files:\n",
        "        if f.endswith('.csv'):\n",
        "            full_path = os.path.join(root, f)\n",
        "            print(\" -> Found CSV:\", full_path)\n",
        "\n",
        "# Choose the correct CSV file path above and load\n",
        "file = full_path  # once identified\n",
        "df = pd.read_csv(file)\n",
        "print(\"\\nINFO:\\n\")\n",
        "print(df.info())\n",
        "print(\"\\nDESCRIBE:\\n\")\n",
        "print(df.describe())\n",
        "print(\"\\nSHAPE:\\n\")\n",
        "print(df.shape)\n",
        "print(\"\\nCOLUMNS:\\n\")\n",
        "print(df.columns)\n",
        "print(\"\\nDATA TYPES:\\n\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nUNIQUE VALUES:\\n\")\n",
        "print(df.nunique())\n",
        "print(\"\\nHEAD:\\n\")\n",
        "print(df.head())\n",
        "print(\"\\nTAIL\\n:\")\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC3UsTi1n14q",
        "outputId": "23cfaef5-fea3-4b1e-ea2c-90365a89c52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NULL VALUES:\n",
            "Product_Code            0\n",
            "Warehouse               0\n",
            "Product_Category        0\n",
            "Date                11239\n",
            "Order_Demand            0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-956463028.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Date'].fillna(median_date, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NULL VALUES AFTER FILLING:\n",
            "Product_Code        0\n",
            "Warehouse           0\n",
            "Product_Category    0\n",
            "Date                0\n",
            "Order_Demand        0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"NULL VALUES:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Convert 'Date' column to datetime objects, coercing errors to NaT\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "\n",
        "# Calculate the median date (or choose another method like forward fill or a constant date)\n",
        "median_date = df['Date'].median()\n",
        "\n",
        "# Fill missing values in 'Date' column with the median date\n",
        "df['Date'].fillna(median_date, inplace=True)\n",
        "\n",
        "print(\"\\nNULL VALUES AFTER FILLING:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D7qUW83zHa9",
        "outputId": "df4b2d9a-374e-44e9-ce88-1e8ed44bc1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8389, 5)\n",
            "(2097, 5)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.20, random_state=0)\n",
        "\n",
        "train_df = train_df.sample(frac=0.01, random_state=0)\n",
        "test_df = test_df.sample(frac=0.01, random_state=0)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2VVfTFUYg1Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfElpwVdYk6k"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Batti-ai/Beepseek-R1\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84E_WQSCwEVw"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# # Check GPU compatibility with bfloat16\n",
        "# if compute_dtype == torch.float16 and use_4bit:\n",
        "#     major, _ = torch.cuda.get_device_capability()\n",
        "#     if major >= 8:\n",
        "#         print(\"=\" * 80)\n",
        "#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "#         print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYeoGXBW6NRG"
      },
      "outputs": [],
      "source": [
        "# Concatenate relevant columns into a single text column for tokenization\n",
        "train_df['text'] = train_df['Product_Code'] + \" \" + train_df['Warehouse'] + \" \" + train_df['Product_Category'] + \" \" + train_df['Date'].astype(str) + \" \" + train_df['Order_Demand']\n",
        "test_df['text'] = test_df['Product_Code'] + \" \" + test_df['Warehouse'] + \" \" + test_df['Product_Category'] + \" \" + test_df['Date'].astype(str) + \" \" + test_df['Order_Demand']\n",
        "\n",
        "# Tokenize the text column\n",
        "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3eHWkcT2HV0"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Create Hugging Face datasets from tokenized encodings\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "\n",
        "# Test Data\n",
        "test_dataset = Dataset.from_dict(test_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJDi4sL2YnOy"
      },
      "outputs": [],
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHERMqfKYqrJ"
      },
      "outputs": [],
      "source": [
        "# trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJNhkphCYsN7"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"what is the demand of product_1798?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "# print(result[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQWB5yEZGcG8NEbXKPPCk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}